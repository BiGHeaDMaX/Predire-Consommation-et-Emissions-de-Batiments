{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBRegressor (XGBoost)**\n",
    "\n",
    "*Fonctionne avec Python 3.10.9 (Anaconda 23.3.1) et Keras 2.10.0*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importation des bibliothèques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour les traîtements sur les variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Modèle de ML utilisé\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# Si on veut optimiser adam, notamment le learning rate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Pour gridsearch\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Pour mesures des metrics des résultats\n",
    "from sklearn import metrics\n",
    "\n",
    "# Pour mise en forme des résultats\n",
    "import colorama\n",
    "\n",
    "# Désactiver un warning DeprecationWarning: KerasRegressor is deprecated\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importation du dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_a_utiliser = \"data_clean.xlsx\"\n",
    "\n",
    "data = pd.read_excel(dataset_a_utiliser)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fonctions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale(dataset_name, y_column, features_list, numeric_features, categorical_features, stratification):\n",
    "\n",
    "    ##############################\n",
    "    # Split du dataset en X et y #\n",
    "    ##############################\n",
    "\n",
    "    global X\n",
    "    global y\n",
    "\n",
    "    X = data.loc[:,features_list] # On ajoute nos features dans le X\n",
    "    y = data.loc[:,y_column] # On ajoute ce qu'on veut prédire dans le y\n",
    "\n",
    "    #logtransformer = FunctionTransformer(np.log, inverse_func = np.exp, check_inverse = True)\n",
    "    #y = logtransformer.transform(y)\n",
    "\n",
    "    ############################\n",
    "    # Scaling et Encoding de X #\n",
    "    ############################\n",
    "\n",
    "    global feature_encoder\n",
    "\n",
    "    # Ici RobustScaler() améliore un peu les résultats\n",
    "    numeric_transformer = StandardScaler()\n",
    "\n",
    "    categorical_transformer = OneHotEncoder()\n",
    "\n",
    "    feature_encoder = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('cat', categorical_transformer, categorical_features),    \n",
    "                    ('num', numeric_transformer, numeric_features)\n",
    "                    ]\n",
    "                )\n",
    "    X = feature_encoder.fit_transform(X)\n",
    "\n",
    "\n",
    "    ####################################\n",
    "    # Split de X et y en train et test #\n",
    "    ####################################\n",
    "\n",
    "    global X_train\n",
    "    global X_test\n",
    "    global y_train\n",
    "    global y_test\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=0,\n",
    "                                                        stratify=eval(stratification))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Choix des features #\n",
    "######################\n",
    "\n",
    "dataset_name = data\n",
    "\n",
    "# Si rien mettre []\n",
    "categorical_features = [\n",
    "                        'BuildingType', \n",
    "                        'PrimaryPropertyType',\n",
    "                        'Neighborhood',\n",
    "                        'ZipCode',\n",
    "                        #'YearBuilt' # Meilleurs résultats sans\n",
    "                       ]\n",
    "\n",
    "# Si rien mettre []\n",
    "numeric_features = [\n",
    "                    'NumberofBuildings',\n",
    "                    'NumberofFloors',\n",
    "                    'PropertyGFAParking',\n",
    "                    'PropertyGFABuilding(s)',\n",
    "                    'Latitude',\n",
    "                    'Longitude',\n",
    "                    #'age_bat' # meilleur score sans\n",
    "                   ]\n",
    "\n",
    "# Toutes les features\n",
    "features_list = categorical_features + numeric_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Keras***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== [SiteEnergyUse_kBtu] ===========\n",
      "--- Entraînement simple : ---\n",
      "Scores du modèle pour prédire SiteEnergyUse_kBtu :\n",
      "R² : \u001b[1m\u001b[42m\u001b[30m -0.332 \u001b[0m\n",
      "MSE : 2.418e+14\n",
      "MAE : 7.762e+06\n",
      "--- GridSearch : ---\n",
      "Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement (): {'batch_size': 25, 'epochs': 30}\n",
      "Score sur le jeu de test (, avec paramètres optimaux) :\n",
      "R² : \u001b[1m\u001b[42m\u001b[30m 0.572 \u001b[0m\n",
      "MSE : 7.765e+13\n",
      "MAE : 3.679e+06\n",
      "________________________________________________________________________________ \n",
      "\n",
      "=========== [TotalGHGEmissions] ===========\n",
      "--- Entraînement simple : ---\n",
      "Scores du modèle pour prédire TotalGHGEmissions :\n",
      "R² : \u001b[1m\u001b[42m\u001b[30m -0.040 \u001b[0m\n",
      "MSE : 9.907e+04\n",
      "MAE : 126.7\n",
      "--- GridSearch : ---\n",
      "Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement (): {'batch_size': 10, 'epochs': 50}\n",
      "Score sur le jeu de test (, avec paramètres optimaux) :\n",
      "R² : \u001b[1m\u001b[42m\u001b[30m 0.638 \u001b[0m\n",
      "MSE : 3.447e+04\n",
      "MAE : 90.61\n",
      "________________________________________________________________________________ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Targets à estimer\n",
    "targets = ['SiteEnergyUse_kBtu', 'TotalGHGEmissions']\n",
    "\n",
    "# \"y\" pour stratifier y, sinon \"None\"\n",
    "stratification = \"None\"\n",
    "\n",
    "# Afficher les résultats détaillés (True/False)\n",
    "details = False\n",
    "\n",
    "for i in targets:\n",
    "\n",
    "    y_column=i\n",
    "\n",
    "    print(f\"=========== [{i}] ===========\")\n",
    "\n",
    "    # Split & Scale du dataset\n",
    "    split_and_scale(dataset_name, y_column, features_list, numeric_features, categorical_features, stratification)\n",
    "\n",
    "    X_train = X_train.toarray()\n",
    "\n",
    "\n",
    "    ##################################\n",
    "    # Création du réseau de neurones #\n",
    "    ##################################\n",
    "\n",
    "\n",
    "    def initialize_model():\n",
    "        \n",
    "\n",
    "        model = models.Sequential()\n",
    "    #On indique à notre modèle la dimension des données d'entrées qui correspond au nombre de colonnes de X_train\n",
    "        model.add(keras.Input(shape=(X_train.shape[1]))) \n",
    "    #On met la première couche de notre réseau de neurones. Nous avons une couche avec 140 perceptrons car nous avons\n",
    "    #140 colonnes dans nos données d'entrée comme dit plus haut. La fonction sigmoid est particulièrement indiquée\n",
    "    #pour les problèmes de classification. Nous vous encourageons à aller voir à quoi elle ressemble. \n",
    "    #    model.add(layers.Dense(86, input_dim=2, activation='sigmoid'))\n",
    "    #Nous ajoutons une seconde couche car nous sommes dans un problème non linéaire comme dit dans le cours\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish')) #, input_dim=2, activation='sigmoid'\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "        model.add(layers.Dense(X_train.shape[1], activation='swish'))\n",
    "\n",
    "\n",
    "        # elu 605\n",
    "        # relu 609\n",
    "        # selu 620\n",
    "        # softplus 603\n",
    "        # swish 634\n",
    "\n",
    "        model.add(layers.Dense(1)) #, activation='sigmoid')\n",
    "\n",
    "\n",
    "        optimizer = Adam(\n",
    "            learning_rate=0.001,\n",
    "            decay=0.0,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-09,\n",
    "            amsgrad=False\n",
    "        )\n",
    "\n",
    "        # valeurs par défaut : \n",
    "        #'learning_rate': 0.001,\n",
    "        #'decay': 0.0,\n",
    "        #'beta_1': 0.9,\n",
    "        #'beta_2': 0.999,\n",
    "        #'epsilon': 1e-07,\n",
    "        # 'amsgrad': False\n",
    "\n",
    "        \n",
    "    #Ici, nous pouvons ajouter des paramètres à notre modèle. Il faut juste retenir que \"accuracy\" permet d'avoir\n",
    "    #la précision de notre modèle et est particulièrement indiqué pour les problèmes de classification. \n",
    "        model.compile(metrics=['mse'],\n",
    "                    loss='mean_absolute_error',\n",
    "                    optimizer=optimizer\n",
    "                    )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    model = initialize_model()\n",
    "\n",
    "    #\"summary\" appliqué à notre modèle nous permet d'avoir les paramètres qui la compose ainsi que les dimensions de\n",
    "    #notre échantillon à la sortie de chaque couche\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    # Quelques infos supplémentaires sur l'optimiser choisi, notamment le learning_rate\n",
    "    #model.optimizer.get_config()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = initialize_model()\n",
    "\n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        #epochs = 50,\n",
    "                        #batch_size = 25,\n",
    "                        verbose = 0)\n",
    "    \n",
    "    y_pred = model.predict(X_test, verbose = 0)\n",
    "\n",
    "    print(\"--- Entraînement simple : ---\")\n",
    "    print(f\"Scores du modèle pour prédire {i} :\")\n",
    "    print(f\"R² : {colorama.Style.BRIGHT}{colorama.Back.GREEN}{colorama.Fore.BLACK} {metrics.r2_score(y_test, y_pred):.3f} {colorama.Style.RESET_ALL}\")\n",
    "    print(f\"MSE : {metrics.mean_squared_error(y_test, y_pred):.4}\")\n",
    "    print(f\"MAE : {metrics.mean_absolute_error(y_test, y_pred):.4}\")\n",
    "\n",
    "    print(\"--- GridSearch : ---\")\n",
    "    model_grid = KerasRegressor(build_fn=initialize_model, verbose=0)\n",
    "\n",
    "    # Fixer les valeurs des hyperparamètres à tester\n",
    "    param_grid = {'epochs':[30, 50], 'batch_size':[10, 25]}\n",
    "\n",
    "    # Déterminer le score qu'on veut optimiser\n",
    "    score = 'r2'\n",
    "\n",
    "    # Je le sors ici car je vais réutiliser cette valeur plus tard\n",
    "    cv = 5\n",
    "\n",
    "    grid = model_selection.GridSearchCV(\n",
    "        model_grid, # On indique le modèle à tester\n",
    "        param_grid,     # hyperparamètres à tester\n",
    "        cv=cv,           # nombre de folds de validation croisée\n",
    "        scoring=score   # score à optimiser\n",
    "    )\n",
    "\n",
    "    # Optimiser ce modèle sur le jeu d'entraînement\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # Affichage des résultats #\n",
    "    ###########################\n",
    "\n",
    "    # Afficher le(s) hyperparamètre(s) optimaux\n",
    "    print(f\"Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement (): {grid.best_params_}\")\n",
    "\n",
    "    y_pred = grid.predict(X_test)\n",
    "    print(f\"Score sur le jeu de test (, avec paramètres optimaux) :\")\n",
    "    print(f\"R² : {colorama.Style.BRIGHT}{colorama.Back.GREEN}{colorama.Fore.BLACK} {metrics.r2_score(y_test, y_pred):.3f} {colorama.Style.RESET_ALL}\")\n",
    "    print(f\"MSE : {metrics.mean_squared_error(y_test, y_pred):.4}\")\n",
    "    print(f\"MAE : {metrics.mean_absolute_error(y_test, y_pred):.4}\")\n",
    "    print('_'*80, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
